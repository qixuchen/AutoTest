# Use Docker image to reproduce data/code/experiments in AutoTest:

## Overview:

This file contains instructions to use a prepared docker image (self-contained with data, code and all dependencies), to both reproduce our results and to facilitate future research. 

This docker image was tested on a Ubuntu 20.04 with 64-core 2.4G CPU and 512G memory. 

Please install docker via the [offical instructions](https://docs.docker.com/engine/install/ubuntu/) and make sure docker services are running.


## Start Docker image:
Assume that your working directory is `{work_dir}`.

- **Load Image**: Load the docker image file, and create a directory called `results` where the output will be stored. (The load step may take several minutes, as we have a large docker image with all data and code).

        sudo docker load < autotest_docker.tar.gz

        mkdir results

- **Start Docker**: Start the docker in interactive mode. Here we also link the `results` directory just created on the host file system to `/root/AutoTest/results` in the container's file system, so that the output generated by the container can be also stored and viewed on the host.

        sudo docker run --rm -it --mount type=bind,source="$(pwd)"/results,target=/root/AutoTest/results autotest_docker

- **Setup Python for AutoTest**: You will see the interactive command line of the running Docker image. The following lines are for setting up AutoTest:

        conda activate root/py37

        cd root/AutoTest

        python3 AutoTest_docker_setup.py


## Reproduce main results in the paper with one script

- **Reproduce main PR curve results**: Run the following command to quickly reproduce the main results (PR-curves) in our paper. We have stored the required intermediate results so it only takes a few minutes.

    python3 AutoTest_reproduce_main_results.py 

- **Inspect results**: After it finishes, the PR-curves can be found in `{work_dir}/results/pr_curve`, where `{work_dir}` is your working directory.

## Run AutoTest Step-by-step

AutoTest consists of 3 sequential steps. One may also run the entire process of AutoTest step-by-step, to modify and customize as needed:  



- The first step is to generate the SDC candidates:

        python3 STEP1_SDC_generation.py <train_corpus_name>

    where `<train_corpus_name>` is the name of one of the training corpora: `rt_train`, `st_train`, and `tablib`. 
Be reminded that this step may take 100 - 150 hours to complete, and may require 200 - 300 GB disk space. 

- Then next step is SDC selection:

        python3 STEP2_SDC_selection.py <train_corpus_name>

    After this step, the selected SDCs (in human-readable format) can be found in `{work_dir}/results/SDC/<train_corpus_name>_selected_sdc.csv`.
    The selection step can take 2 -3 hours to complete.

- Finally, apply the learnt SDCs on a benchmark for outlier detection:

        python3 STEP3_SDC_application.py <benchmark_name> <train_corpus_name>

    where `<benchmark_name>` is the name of one of the benchmarks: `rt_bench` or `st_bench`.

    `<train_corpus_name>` is the name of the training corpus on which SDCs are learnt (`rt_train`, `st_train`, or `tablib`). 

    The detected outliers can be found in `{work_dir}/results/detected_outliers/<train_corpus_name>_learnt_sdc_on_<benchmark_name>.csv`.

